{"cells":[{"source":"![mobydick](mobydick.jpg)","metadata":{},"id":"b1309988-b429-4fb0-8c4c-193582dbec93","cell_type":"markdown"},{"source":"In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n\nThe Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg.","metadata":{},"id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","cell_type":"markdown"},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n'''\nWhat are the most frequent words in Herman Melville's novel Moby Dick, and how often do they occur?\n\nNote that the HTML file you are asked to request is a cashed version of this file from Project Gutenberg.\n\nYour project will follow these steps:\n\nThe first step will be to request the Moby Dick HTML file using requests and encoding it to utf-8. Here is the URL to scrape from: https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\nNext, you'll extract the HTML and create a BeautifulSoup object using an HTML parser to get the text.\nFollowing that, you'll initialize a regex tokenizer object tokenizer using nltk.tokenize.RegexpTokenizer to keep only alphanumeric text, assigning the results to tokens.\nYou'll transform the tokens into lowercase, removing English stop words, and saving the results to words_no_stop.\nFinally, you'll initialize a Counter object and find the ten most common words, saving the result to top_ten and printing to see what they are.\n'''\n#Start coding here... \n#Get the Moby Dick HTML  \nr = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n# Set the correct text encoding of the HTML page\nr.encoding = 'utf-8'\n# Extract the HTML from the request object\nhtml = r.text\n# Print the first 2000 characters in html\nprint(html[0:2000])\n\n# Create a BeautifulSoup object from the HTML\nhtml_soup = BeautifulSoup(html, \"html.parser\")\n# Get the text out of the soup\nmoby_text = html_soup.get_text()\n\n# Create a tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n# Tokenize the text\ntokens = tokenizer.tokenize(moby_text)\n# Create a list called words containing all tokens transformed to lowercase\nwords = [token.lower() for token in tokens]\n# Print out the first eight words\nwords[:8]\n\n# Get the English stop words from nltk\nstop_words = nltk.corpus.stopwords.words('english')\n# Print out the first eight stop words\nstop_words[:8]\n# Create a list words_ns containing all words that are in words but not in stop_words\nwords_no_stop = [word for word in words if word not in stop_words]\n# Print the first five words_no_stop to check that stop words are gone\nwords_no_stop[:5]\n\n# Initialize a Counter object from our processed list of words\ncount = Counter(words_no_stop)\n# Store ten most common words and their counts as top_ten\ntop_ten = count.most_common(10)\n# Print the top ten words and their counts\nprint(top_ten)","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":909,"lastSuccessfullyExecutedCode":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n'''\nWhat are the most frequent words in Herman Melville's novel Moby Dick, and how often do they occur?\n\nNote that the HTML file you are asked to request is a cashed version of this file from Project Gutenberg.\n\nYour project will follow these steps:\n\nThe first step will be to request the Moby Dick HTML file using requests and encoding it to utf-8. Here is the URL to scrape from: https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\nNext, you'll extract the HTML and create a BeautifulSoup object using an HTML parser to get the text.\nFollowing that, you'll initialize a regex tokenizer object tokenizer using nltk.tokenize.RegexpTokenizer to keep only alphanumeric text, assigning the results to tokens.\nYou'll transform the tokens into lowercase, removing English stop words, and saving the results to words_no_stop.\nFinally, you'll initialize a Counter object and find the ten most common words, saving the result to top_ten and printing to see what they are.\n'''\n#Start coding here... \n#Get the Moby Dick HTML  \nr = requests.get('https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm')\n# Set the correct text encoding of the HTML page\nr.encoding = 'utf-8'\n# Extract the HTML from the request object\nhtml = r.text\n# Print the first 2000 characters in html\nprint(html[0:2000])\n\n# Create a BeautifulSoup object from the HTML\nhtml_soup = BeautifulSoup(html, \"html.parser\")\n# Get the text out of the soup\nmoby_text = html_soup.get_text()\n\n# Create a tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n# Tokenize the text\ntokens = tokenizer.tokenize(moby_text)\n# Create a list called words containing all tokens transformed to lowercase\nwords = [token.lower() for token in tokens]\n# Print out the first eight words\nwords[:8]\n\n# Get the English stop words from nltk\nstop_words = nltk.corpus.stopwords.words('english')\n# Print out the first eight stop words\nstop_words[:8]\n# Create a list words_ns containing all words that are in words but not in stop_words\nwords_no_stop = [word for word in words if word not in stop_words]\n# Print the first five words_no_stop to check that stop words are gone\nwords_no_stop[:5]\n\n# Initialize a Counter object from our processed list of words\ncount = Counter(words_no_stop)\n# Store ten most common words and their counts as top_ten\ntop_ten = count.most_common(10)\n# Print the top ten words and their counts\nprint(top_ten)","executionCancelledAt":null,"lastExecutedAt":1705376891310,"lastScheduledRunId":null,"outputsMetadata":{"0":{"height":57,"type":"stream"},"1":{"height":616,"type":"stream"}}},"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},{"output_type":"stream","name":"stdout","text":"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n<!DOCTYPE html\n   PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n   \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" >\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n  <head>\n    <title>\n      Moby Dick; Or the Whale, by Herman Melville\n    </title>\n    <style type=\"text/css\" xml:space=\"preserve\">\n\n    body { background:#faebd0; color:black; margin-left:15%; margin-right:15%; text-align:justify }\n    P { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\n    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\n    hr  { width: 50%; text-align: center;}\n    .foot { margin-left: 20%; margin-right: 20%; text-align: justify; text-indent: -3em; font-size: 90%; }\n    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\n    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\n    .toc       { margin-left: 10%; margin-bottom: .75em;}\n    .toc2      { margin-left: 20%;}\n    div.fig    { display:block; margin:0 auto; text-align:center; }\n    div.middle { margin-left: 20%; margin-right: 20%; text-align: justify; }\n    .figleft   {float: left; margin-left: 0%; margin-right: 1%;}\n    .figright  {float: right; margin-right: 0%; margin-left: 1%;}\n    .pagenum   {display:inline; font-size: 70%; font-style:normal;\n               margin: 0; padding: 0; position: absolute; right: 1%;\n               text-align: right;}\n    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\n\n    table      {margin-left: 10%;}\n\na:link {color:blue;\n\t\ttext-decoration:none}\nlink {color:blue;\n\t\ttext-decoration:none}\na:visited {color:blue;\n\t\ttext-decoration:none}\na:hover {color:red}\n\n</style>\n  </head>\n  <body>\n<pre xml:space=\"preserve\">\n\nThe Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\n\nThis eBook is for the use of anyone anywh\n[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}