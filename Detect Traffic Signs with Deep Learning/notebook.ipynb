{"cells":[{"source":"![Traffic lights over urban intersection.](traffic.jpg)","metadata":{},"id":"9bef1e92-37c0-4f79-a27a-74f017644cdf","cell_type":"markdown"},{"source":"**Challenges in Traffic Sign Management**\n\nTraffic signs convey vital information to drivers. However, their effectiveness can be hindered by various factors, such as their visibility under different lighting conditions or the presence of obstructions.\n\n\n**Training Traffic Sign Detection Models**\n\nTo address these challenges and enhance stop sign and traffic light detection capabilities, advanced technologies such as deep learning and computer vision have gained significant attention. In this project, you'll train an object detection model on 6 images of stop signs and 6 images of traffic lights, taken from various angles and lighting conditions. This training teaches the model to classify and locate these signs in images, improving its robustness and reliability for real-world applications.","metadata":{},"id":"1c2bc1d6-3780-4d6f-8016-c90898817887","cell_type":"markdown"},{"source":"# Import required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n\n# Load preprocessed images and the corresponding labels\nimage, labels = np.load('batch.npy',allow_pickle=True).tolist()\n\n# hyperparameters\ninput_size = image.shape[1] # dimension of input image\nnum_classes = labels['classifier_head'].shape[1] # number of classes\nDROPOUT_FACTOR = 0.2 # dropout probability\n\n# visualize one example preprocessed image\nplt.imshow(image[2])\nplt.axis(\"off\")","metadata":{"executionCancelledAt":null,"executionTime":7525,"lastExecutedAt":1719638292892,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import required libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\n\n# Load preprocessed images and the corresponding labels\nimage, labels = np.load('batch.npy',allow_pickle=True).tolist()\n\n# hyperparameters\ninput_size = image.shape[1] # dimension of input image\nnum_classes = labels['classifier_head'].shape[1] # number of classes\nDROPOUT_FACTOR = 0.2 # dropout probability\n\n# visualize one example preprocessed image\nplt.imshow(image[2])\nplt.axis(\"off\")","outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":185,"type":"stream"}},"lastExecutedByKernel":"9daee5db-0a8f-4f0b-9ac2-909117aac799"},"id":"2f26fc58-2bcd-45a6-841f-cbcdd391e2f2","cell_type":"code","execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":"(-0.5, 223.5, 223.5, -0.5)"},"metadata":{},"execution_count":1},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{"image/png":{"width":389,"height":389}}}]},{"source":"# Start coding here\n# Use as many cells as you need\n'''\nAddress the challenges faced in modern transportation systems using Keras.\n\nDefine and train an object detection model to identify traffic signs and lights. Save the training accuracy in a variable named accuracy.\nOnly run your training loop for 20 epochs due to the small size of the training data.\n'''\n# Create a sequential model - a linear stack of layers\nmodel = keras.Sequential()\n\n# Feature extractor: This part of the model extracts features from the input images\n# Convolutional layer with 16 filters, each filter with a size of 3x3, using ReLU activation function\n# Input shape is the size of the input image: (input_size, input_size, 3) - 3 channels for RGB images\nmodel.add(keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(input_size, input_size, 3)))\n# Average pooling layer to reduce spatial dimensions by taking the average value in each 2x2 patch\nmodel.add(keras.layers.AveragePooling2D(2, 2))\n# Add another convolutional layer with 32 filters and ReLU activation\nmodel.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu'))\n# Another average pooling layer\nmodel.add(keras.layers.AveragePooling2D(2, 2))\n# Add one more convolutional layer with 64 filters and ReLU activation\nmodel.add(keras.layers.Conv2D(64, kernel_size=3, activation='relu'))\n# Dropout layer to prevent overfitting by randomly dropping a fraction of the units during training\nmodel.add(keras.layers.Dropout(DROPOUT_FACTOR))\n# Another average pooling layer\nmodel.add(keras.layers.AveragePooling2D(2, 2))\n\n# Model adaptor: This part of the model adapts the extracted features for classification\n# Flatten layer to convert the 3D feature maps into a 1D feature vector\nmodel.add(keras.layers.Flatten())\n# Dense (fully connected) layer with 64 neurons and ReLU activation\nmodel.add(keras.layers.Dense(64, activation='relu'))\n\n# Classifier head: This part of the model performs the actual classification\n# Dense layer with 64 neurons and ReLU activation\nmodel.add(keras.layers.Dense(64, activation='relu'))\n# Output layer with 'num_classes' neurons and softmax activation, representing class probabilities\nmodel.add(keras.layers.Dense(num_classes, activation='softmax', name='classifier_head'))\n\n# Compile the model: Define optimizer, loss function, and evaluation metrics\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model using the data generator\n# 'image' contains the input images, 'labels' contains corresponding labels\n# Epochs determine how many times the entire dataset is passed forward and backward through the network\nhistory = model.fit(image, labels['classifier_head'], epochs=20)\n\n# Print the training accuracy\naccuracy = history.history['accuracy'][-1]\nprint(f\"Training accuracy: {accuracy}\")\n","metadata":{"executionCancelledAt":null,"executionTime":8382,"lastExecutedAt":1719638301274,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\n# Use as many cells as you need\n'''\nAddress the challenges faced in modern transportation systems using Keras.\n\nDefine and train an object detection model to identify traffic signs and lights. Save the training accuracy in a variable named accuracy.\nOnly run your training loop for 20 epochs due to the small size of the training data.\n'''\n# Create a sequential model - a linear stack of layers\nmodel = keras.Sequential()\n\n# Feature extractor: This part of the model extracts features from the input images\n# Convolutional layer with 16 filters, each filter with a size of 3x3, using ReLU activation function\n# Input shape is the size of the input image: (input_size, input_size, 3) - 3 channels for RGB images\nmodel.add(keras.layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(input_size, input_size, 3)))\n# Average pooling layer to reduce spatial dimensions by taking the average value in each 2x2 patch\nmodel.add(keras.layers.AveragePooling2D(2, 2))\n# Add another convolutional layer with 32 filters and ReLU activation\nmodel.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu'))\n# Another average pooling layer\nmodel.add(keras.layers.AveragePooling2D(2, 2))\n# Add one more convolutional layer with 64 filters and ReLU activation\nmodel.add(keras.layers.Conv2D(64, kernel_size=3, activation='relu'))\n# Dropout layer to prevent overfitting by randomly dropping a fraction of the units during training\nmodel.add(keras.layers.Dropout(DROPOUT_FACTOR))\n# Another average pooling layer\nmodel.add(keras.layers.AveragePooling2D(2, 2))\n\n# Model adaptor: This part of the model adapts the extracted features for classification\n# Flatten layer to convert the 3D feature maps into a 1D feature vector\nmodel.add(keras.layers.Flatten())\n# Dense (fully connected) layer with 64 neurons and ReLU activation\nmodel.add(keras.layers.Dense(64, activation='relu'))\n\n# Classifier head: This part of the model performs the actual classification\n# Dense layer with 64 neurons and ReLU activation\nmodel.add(keras.layers.Dense(64, activation='relu'))\n# Output layer with 'num_classes' neurons and softmax activation, representing class probabilities\nmodel.add(keras.layers.Dense(num_classes, activation='softmax', name='classifier_head'))\n\n# Compile the model: Define optimizer, loss function, and evaluation metrics\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model using the data generator\n# 'image' contains the input images, 'labels' contains corresponding labels\n# Epochs determine how many times the entire dataset is passed forward and backward through the network\nhistory = model.fit(image, labels['classifier_head'], epochs=20)\n\n# Print the training accuracy\naccuracy = history.history['accuracy'][-1]\nprint(f\"Training accuracy: {accuracy}\")\n","lastExecutedByKernel":"9daee5db-0a8f-4f0b-9ac2-909117aac799","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":101,"type":"stream"},"2":{"height":412,"type":"stream"}}},"id":"84cdd85c-afb2-4f4f-b66c-8b158eeac8d2","cell_type":"code","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch 1/20\n"},{"output_type":"stream","name":"stderr","text":"2024-06-29 05:18:12.904036: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"},{"output_type":"stream","name":"stdout","text":"1/1 [==============================] - 1s 926ms/step - loss: 3.4779 - accuracy: 0.3333\nEpoch 2/20\n1/1 [==============================] - 0s 400ms/step - loss: 58.1147 - accuracy: 0.5000\nEpoch 3/20\n1/1 [==============================] - 0s 402ms/step - loss: 144.5329 - accuracy: 0.5000\nEpoch 4/20\n1/1 [==============================] - 0s 389ms/step - loss: 71.3714 - accuracy: 0.5000\nEpoch 5/20\n1/1 [==============================] - 0s 383ms/step - loss: 8.3706 - accuracy: 0.7500\nEpoch 6/20\n1/1 [==============================] - 0s 398ms/step - loss: 0.9800 - accuracy: 0.8333\nEpoch 7/20\n1/1 [==============================] - 0s 394ms/step - loss: 6.3997 - accuracy: 0.5000\nEpoch 8/20\n1/1 [==============================] - 0s 338ms/step - loss: 2.6222 - accuracy: 0.6667\nEpoch 9/20\n1/1 [==============================] - 0s 334ms/step - loss: 0.0101 - accuracy: 1.0000\nEpoch 10/20\n1/1 [==============================] - 0s 381ms/step - loss: 0.0081 - accuracy: 1.0000\nEpoch 11/20\n1/1 [==============================] - 0s 391ms/step - loss: 0.1803 - accuracy: 0.9167\nEpoch 12/20\n1/1 [==============================] - 0s 395ms/step - loss: 0.0332 - accuracy: 1.0000\nEpoch 13/20\n1/1 [==============================] - 0s 331ms/step - loss: 0.0096 - accuracy: 1.0000\nEpoch 14/20\n1/1 [==============================] - 0s 393ms/step - loss: 0.0015 - accuracy: 1.0000\nEpoch 15/20\n1/1 [==============================] - 0s 383ms/step - loss: 1.7742e-04 - accuracy: 1.0000\nEpoch 16/20\n1/1 [==============================] - 0s 392ms/step - loss: 1.0104e-04 - accuracy: 1.0000\nEpoch 17/20\n1/1 [==============================] - 0s 393ms/step - loss: 7.4978e-04 - accuracy: 1.0000\nEpoch 18/20\n1/1 [==============================] - 0s 331ms/step - loss: 0.0014 - accuracy: 1.0000\nEpoch 19/20\n1/1 [==============================] - 0s 376ms/step - loss: 7.7202e-04 - accuracy: 1.0000\nEpoch 20/20\n1/1 [==============================] - 0s 394ms/step - loss: 1.2000e-05 - accuracy: 1.0000\nTraining accuracy: 1.0\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}